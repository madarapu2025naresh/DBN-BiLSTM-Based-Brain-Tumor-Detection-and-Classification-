{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38bebb9",
   "metadata": {},
   "source": [
    "# Brain Tumor MRI Research Notebook\n",
    "*Generated 2025-08-11 09:59*\n",
    "\n",
    "This notebook provides a complete, modular pipeline for brain tumor detection and classification using MRI images. It includes data intake, preprocessing, exploratory analysis, training (transfer learning baseline), evaluation metrics, confusion matrix, ROC, and Grad‑CAM explainability. Slots are provided to plug in advanced models (e.g., DBN + BiLSTM) if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127aae6",
   "metadata": {},
   "source": [
    "## 0. Environment & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fff292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on a clean environment, uncomment the next cell to install packages.\n",
    "# %pip install -q numpy pandas matplotlib scikit-learn scikit-image opencv-python pillow tqdm seaborn tensorflow==2.15.0\n",
    "# If you plan to use PyTorch instead of TensorFlow, you can install it and swap the model section accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json, itertools, random, shutil, zipfile, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, roc_auc_score,\n",
    "                             roc_curve, auc)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For image utilities\n",
    "from PIL import Image\n",
    "\n",
    "# Optional: uncomment if you use these paths\n",
    "# import cv2\n",
    "from skimage import exposure, filters, morphology, measure\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cc071",
   "metadata": {},
   "source": [
    "## 1. Dataset Setup\n",
    "Organize your dataset in the following directory structure (typical for Brain Tumor MRI datasets):\n",
    "```\n",
    "dataset_root/\n",
    "  train/\n",
    "    glioma/\n",
    "    meningioma/\n",
    "    pituitary/\n",
    "    notumor/\n",
    "  val/\n",
    "    glioma/\n",
    "    meningioma/\n",
    "    pituitary/\n",
    "    notumor/\n",
    "  test/\n",
    "    glioma/\n",
    "    meningioma/\n",
    "    pituitary/\n",
    "    notumor/\n",
    "```\n",
    "Update `DATASET_ROOT` below to point to your local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd37b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configure paths ===\n",
    "DATASET_ROOT = Path(\"/path/to/brain-tumor-dataset\")  # <-- CHANGE THIS\n",
    "IMG_SIZE = (224, 224)   # MobileNetV2 default; adjust if needed\n",
    "BATCH_SIZE = 32\n",
    "CLASSES = [\"glioma\", \"meningioma\", \"pituitary\", \"notumor\"]\n",
    "\n",
    "assert len(CLASSES) >= 2, \"Need at least two classes\"\n",
    "\n",
    "# Sanity check (won't fail if path missing; will be validated when loading)\n",
    "print(\"Dataset root:\", DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308bd7e",
   "metadata": {},
   "source": [
    "### 1.1 Quick File Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa37170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(root: Path, classes):\n",
    "    rows = []\n",
    "    for split in [\"train\",\"val\",\"test\"]:\n",
    "        for c in classes:\n",
    "            d = root / split / c\n",
    "            n = len(list(d.glob(\"**/*.png\"))) + len(list(d.glob(\"**/*.jpg\"))) + len(list(d.glob(\"**/*.jpeg\")))\n",
    "            rows.append({\"split\":split, \"class\":c, \"count\":n})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "try:\n",
    "    counts_df = count_images(DATASET_ROOT, CLASSES)\n",
    "    display(counts_df)\n",
    "except Exception as e:\n",
    "    print(\"Skipping counts (path may be invalid):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e892b9",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(root, split=\"train\", classes=CLASSES, n_per_class=3, img_size=IMG_SIZE):\n",
    "    fig, axes = plt.subplots(len(classes), n_per_class, figsize=(n_per_class*3, len(classes)*3))\n",
    "    if len(classes) == 1:\n",
    "        axes = np.array([axes])\n",
    "    for i, c in enumerate(classes):\n",
    "        imgs = list((root / split / c).glob(\"**/*\"))\n",
    "        imgs = [p for p in imgs if p.suffix.lower() in [\".png\",\".jpg\",\".jpeg\"]]\n",
    "        random.shuffle(imgs)\n",
    "        for j in range(n_per_class):\n",
    "            ax = axes[i, j] if n_per_class > 1 else axes[i, 0]\n",
    "            if j < len(imgs):\n",
    "                im = Image.open(imgs[j]).convert(\"RGB\").resize(img_size)\n",
    "                ax.imshow(im)\n",
    "                ax.set_title(f\"{c}\")\n",
    "            ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# show_samples(DATASET_ROOT, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc7ee6",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "Here we implement simple intensity normalization, optional CLAHE (contrast enhancement), and Otsu thresholding examples. You can adapt this section for your preferred pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da9eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pil(img: Image.Image, img_size=IMG_SIZE, do_clahe=False):\n",
    "    img = img.convert(\"L\")  # grayscale for intensity ops; keep RGB for CNN below if desired\n",
    "    arr = np.array(img)\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)\n",
    "\n",
    "    if do_clahe:\n",
    "        arr = exposure.equalize_adapthist(arr, clip_limit=0.02)\n",
    "\n",
    "    arr = (arr * 255).astype(np.uint8)\n",
    "    img_out = Image.fromarray(arr).convert(\"RGB\").resize(img_size)\n",
    "    return img_out\n",
    "\n",
    "# Example (commented until a real file path is provided):\n",
    "# sample_path = next((DATASET_ROOT/'train'/'glioma').glob(\"*.jpg\"))\n",
    "# preprocess_pil(Image.open(sample_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a719c076",
   "metadata": {},
   "source": [
    "### 3.1 Simple Segmentation Demo (Otsu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_otsu_segmentation(img: Image.Image):\n",
    "    gray = np.array(img.convert(\"L\"))\n",
    "    thresh = filters.threshold_otsu(gray)\n",
    "    mask = (gray > thresh).astype(np.uint8)\n",
    "    # Clean small regions\n",
    "    mask = morphology.remove_small_objects(mask.astype(bool), min_size=64)\n",
    "    mask = morphology.remove_small_holes(mask, area_threshold=64)\n",
    "    return mask.astype(np.uint8)\n",
    "\n",
    "# Example (requires a sample image path):\n",
    "# sample = Image.open(sample_path).resize(IMG_SIZE)\n",
    "# mask = simple_otsu_segmentation(sample)\n",
    "# plt.figure(figsize=(6,3))\n",
    "# plt.subplot(1,2,1); plt.imshow(sample); plt.title(\"Original\"); plt.axis(\"off\")\n",
    "# plt.subplot(1,2,2); plt.imshow(mask, cmap=\"gray\"); plt.title(\"Otsu Mask\"); plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df2b13",
   "metadata": {},
   "source": [
    "## 4. Dataloaders (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(root, img_size=IMG_SIZE, batch_size=BATCH_SIZE, seed=SEED):\n",
    "    train_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        root/\"train\", image_size=img_size, batch_size=batch_size, label_mode=\"categorical\", seed=seed, shuffle=True)\n",
    "    val_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        root/\"val\", image_size=img_size, batch_size=batch_size, label_mode=\"categorical\", seed=seed, shuffle=False)\n",
    "    test_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        root/\"test\", image_size=img_size, batch_size=batch_size, label_mode=\"categorical\", seed=seed, shuffle=False)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "# train_ds, val_ds, test_ds = make_datasets(DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12017a9",
   "metadata": {},
   "source": [
    "### 4.1 Augmentation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb620108",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065f386",
   "metadata": {},
   "source": [
    "## 5. Model: Transfer Learning Baseline (MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92318835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes=len(CLASSES), img_size=IMG_SIZE):\n",
    "    base = keras.applications.MobileNetV2(\n",
    "        input_shape=img_size + (3,), include_top=False, weights=\"imagenet\")\n",
    "    base.trainable = False  # fine-tune later\n",
    "\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "    x = data_augmentation(inputs)\n",
    "    x = keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "    x = base(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# model = build_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8780aea",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ecccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "# train_ds, val_ds, test_ds = make_datasets(DATASET_ROOT)\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=EPOCHS\n",
    "# )\n",
    "# pd.DataFrame(history.history).to_csv(\"training_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43196b1d",
   "metadata": {},
   "source": [
    "### 6.1 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history_dict):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    acc = history_dict.get(\"accuracy\", [])\n",
    "    val_acc = history_dict.get(\"val_accuracy\", [])\n",
    "    loss = history_dict.get(\"loss\", [])\n",
    "    val_loss = history_dict.get(\"val_loss\", [])\n",
    "    epochs_range = range(1, len(acc)+1)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.plot(epochs_range, acc, label=\"train_acc\")\n",
    "    if val_acc: plt.plot(epochs_range, val_acc, label=\"val_acc\")\n",
    "\n",
    "    # Loss on twin y-axis for clarity\n",
    "    ax2 = plt.gca().twinx()\n",
    "    ax2.plot(epochs_range, loss, linestyle=\"--\", label=\"train_loss\")\n",
    "    if val_loss: ax2.plot(epochs_range, val_loss, linestyle=\"--\", label=\"val_loss\")\n",
    "\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example after training:\n",
    "# plot_curves(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d6c51",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a526bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_ds, class_names=CLASSES):\n",
    "    y_true, y_prob = [], []\n",
    "    for x, y in test_ds:\n",
    "        p = model.predict(x, verbose=0)\n",
    "        y_prob.append(p)\n",
    "        y_true.append(y.numpy())\n",
    "    y_prob = np.concatenate(y_prob, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_labels, y_pred, labels=list(range(len(class_names))))\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(class_names)), class_names)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nClassification Report\\n\")\n",
    "    print(classification_report(y_true_labels, y_pred, target_names=class_names))\n",
    "\n",
    "    # One-vs-Rest ROC-AUC (if at least 2 classes)\n",
    "    if y_prob.shape[1] >= 2:\n",
    "        try:\n",
    "            auc_macro = roc_auc_score(y_true, y_prob, multi_class=\"ovr\")\n",
    "            print(f\"Macro ROC-AUC: {auc_macro:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(\"ROC-AUC could not be computed:\", e)\n",
    "\n",
    "# evaluate_model(model, test_ds, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ae08e",
   "metadata": {},
   "source": [
    "## 8. Explainability: Grad‑CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array, training=False)\n",
    "        class_idx = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, class_idx]\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0) / (np.max(heatmap) + 1e-8)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Example usage after training:\n",
    "# target_layer = [l.name for l in model.layers if isinstance(l, layers.Conv2D)][-1]\n",
    "# img_path = next((DATASET_ROOT/'test'/'glioma').glob(\"*.jpg\"))\n",
    "# img = Image.open(img_path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "# x = np.expand_dims(np.array(img)/255.0, 0).astype(np.float32)\n",
    "# heatmap = make_gradcam_heatmap(x, model, last_conv_layer_name=target_layer)\n",
    "# plt.figure(figsize=(6,3))\n",
    "# plt.subplot(1,2,1); plt.imshow(img); plt.title(\"Image\"); plt.axis(\"off\")\n",
    "# plt.subplot(1,2,2); plt.imshow(heatmap); plt.title(\"Grad-CAM\"); plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3550f4",
   "metadata": {},
   "source": [
    "## 9. (Optional) Advanced Slot: DBN + BiLSTM\n",
    "This section is a placeholder where you can prototype a DBN feature extractor (via stacked RBMs) and feed the extracted features into a BiLSTM classifier over slice sequences. Implementations vary; consider using PyTorch or TensorFlow Probability for RBM-like layers, or substitute with an autoencoder as a practical surrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode / sketch (fill as needed):\n",
    "# 1) Build/Train stacked RBMs (or autoencoder) to extract features from each image/slice.\n",
    "# 2) Aggregate per-patient sequences: shape [time/slices, feature_dim].\n",
    "# 3) Feed sequences to a BiLSTM for classification.\n",
    "# 4) Train end-to-end (optionally fine-tune feature extractor).\n",
    "\n",
    "# from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Input, Masking\n",
    "# seq_inputs = Input(shape=(None, feature_dim))\n",
    "# x = Bidirectional(layers.LSTM(128, return_sequences=False))(seq_inputs)\n",
    "# outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "# seq_model = keras.Model(seq_inputs, outputs)\n",
    "# seq_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f729692",
   "metadata": {},
   "source": [
    "## 10. Export & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save class mapping & configuration\n",
    "config = {\n",
    "    \"classes\": CLASSES,\n",
    "    \"img_size\": IMG_SIZE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "with open(\"run_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"Saved run_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12998adc",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Ensure dataset paths are correct before running the dataloader cell.\n",
    "- Start with the baseline model; once stable, enable fine‑tuning (`base.trainable=True`) and re‑train with a lower LR.\n",
    "- Use the Grad‑CAM cell to sanity‑check that the network is focusing on plausible tumor regions.\n",
    "- Replace MobileNetV2 with any backbone (e.g., ResNet50, EfficientNet) if you prefer.\n",
    "- For publications, record seeds, exact package versions, and hardware.\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Generated by ChatGPT"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
