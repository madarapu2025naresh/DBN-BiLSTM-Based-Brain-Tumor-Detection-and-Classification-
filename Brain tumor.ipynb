{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38bebb9",
   "metadata": {},
   "source": [
    "# Brain Tumor MRI Research Notebook\n",
    "*Generated 2025-08-11 09:59*\n",
    "\n",
    "This notebook provides a complete, modular pipeline for brain tumor detection and classification using MRI images. It includes data intake, preprocessing, exploratory analysis, training (transfer learning baseline), evaluation metrics, confusion matrix, ROC, and Gradâ€‘CAM explainability. Slots are provided to plug in advanced models (e.g., DBN + BiLSTM) if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127aae6",
   "metadata": {},
   "source": [
    "## 0. Environment & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fff292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on a clean environment, uncomment the next cell to install packages.\n",
    "# %pip install -q numpy pandas matplotlib scikit-learn scikit-image opencv-python pillow tqdm seaborn tensorflow==2.15.0\n",
    "# If you plan to use PyTorch instead of TensorFlow, you can install it and swap the model section accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json, itertools, random, shutil, zipfile, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, roc_auc_score,\n",
    "                             roc_curve, auc)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For image utilities\n",
    "from PIL import Image\n",
    "\n",
    "# Optional: uncomment if you use these paths\n",
    "# import cv2\n",
    "from skimage import exposure, filters, morphology, measure\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cc071",
   "metadata": {},
   "source": [
    "## 1. Dataset Setup\n",
    "Organize your dataset in the following directory structure (typical for Brain Tumor MRI datasets):\n",
    "```\n",
    "dataset_root/\n",
    "  train/\n",
    "    glioma/\n",
    "    meningioma/\n",
    "    pituitary/\n",
    "    notumor/\n",
    "  val/\n",
    "    glioma/\n",
    "    meningioma/\n",
    "    pituitary/\n",
    "    notumor/\n",
    "  test/\n",
    "    glioma/\n",
    "    meningioma/\n",
    "    pituitary/\n",
    "    notumor/\n",
    "```\n",
    "Update `DATASET_ROOT` below to point to your local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd37b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configure paths ===\n",
    "DATASET_ROOT = Path(\"/path/to/brain-tumor-dataset\")  # <-- CHANGE THIS\n",
    "IMG_SIZE = (224, 224)   # MobileNetV2 default; adjust if needed\n",
    "BATCH_SIZE = 32\n",
    "CLASSES = [\"glioma\", \"meningioma\", \"pituitary\", \"notumor\"]\n",
    "\n",
    "assert len(CLASSES) >= 2, \"Need at least two classes\"\n",
    "\n",
    "# Sanity check (won't fail if path missing; will be validated when loading)\n",
    "print(\"Dataset root:\", DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308bd7e",
   "metadata": {},
   "source": [
    "### 1.1 Quick File Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa37170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(root: Path, classes):\n",
    "    rows = []\n",
    "    for split in [\"train\",\"val\",\"test\"]:\n",
    "        for c in classes:\n",
    "            d = root / split / c\n",
    "            n = len(list(d.glob(\"**/*.png\"))) + len(list(d.glob(\"**/*.jpg\"))) + len(list(d.glob(\"**/*.jpeg\")))\n",
    "            rows.append({\"split\":split, \"class\":c, \"count\":n})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "try:\n",
    "    counts_df = count_images(DATASET_ROOT, CLASSES)\n",
    "    display(counts_df)\n",
    "except Exception as e:\n",
    "    print(\"Skipping counts (path may be invalid):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e892b9",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(root, split=\"train\", classes=CLASSES, n_per_class=3, img_size=IMG_SIZE):\n",
    "    fig, axes = plt.subplots(len(classes), n_per_class, figsize=(n_per_class*3, len(classes)*3))\n",
    "    if len(classes) == 1:\n",
    "        axes = np.array([axes])\n",
    "    for i, c in enumerate(classes):\n",
    "        imgs = list((root / split / c).glob(\"**/*\"))\n",
    "        imgs = [p for p in imgs if p.suffix.lower() in [\".png\",\".jpg\",\".jpeg\"]]\n",
    "        random.shuffle(imgs)\n",
    "        for j in range(n_per_class):\n",
    "            ax = axes[i, j] if n_per_class > 1 else axes[i, 0]\n",
    "            if j < len(imgs):\n",
    "                im = Image.open(imgs[j]).convert(\"RGB\").resize(img_size)\n",
    "                ax.imshow(im)\n",
    "                ax.set_title(f\"{c}\")\n",
    "            ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# show_samples(DATASET_ROOT, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc7ee6",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "Here we implement simple intensity normalization, optional CLAHE (contrast enhancement), and Otsu thresholding examples. You can adapt this section for your preferred pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da9eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pil(img: Image.Image, img_size=IMG_SIZE, do_clahe=False):\n",
    "    img = img.convert(\"L\")  # grayscale for intensity ops; keep RGB for CNN below if desired\n",
    "    arr = np.array(img)\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)\n",
    "\n",
    "    if do_clahe:\n",
    "        arr = exposure.equalize_adapthist(arr, clip_limit=0.02)\n",
    "\n",
    "    arr = (arr * 255).astype(np.uint8)\n",
    "    img_out = Image.fromarray(arr).convert(\"RGB\").resize(img_size)\n",
    "    return img_out\n",
    "\n",
    "# Example (commented until a real file path is provided):\n",
    "# sample_path = next((DATASET_ROOT/'train'/'glioma').glob(\"*.jpg\"))\n",
    "# preprocess_pil(Image.open(sample_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a719c076",
   "metadata": {},
   "source": [
    "### 3.1 Simple Segmentation Demo (Otsu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_otsu_segmentation(img: Image.Image):\n",
    "    gray = np.array(img.convert(\"L\"))\n",
    "    thresh = filters.threshold_otsu(gray)\n",
    "    mask = (gray > thresh).astype(np.uint8)\n",
    "    # Clean small regions\n",
    "    mask = morphology.remove_small_objects(mask.astype(bool), min_size=64)\n",
    "    mask = morphology.remove_small_holes(mask, area_threshold=64)\n",
    "    return mask.astype(np.uint8)\n",
    "\n",
    "# Example (requires a sample image path):\n",
    "# sample = Image.open(sample_path).resize(IMG_SIZE)\n",
    "# mask = simple_otsu_segmentation(sample)\n",
    "# plt.figure(figsize=(6,3))\n",
    "# plt.subplot(1,2,1); plt.imshow(sample); plt.title(\"Original\"); plt.axis(\"off\")\n",
    "# plt.subplot(1,2,2); plt.imshow(mask, cmap=\"gray\"); plt.title(\"Otsu Mask\"); plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df2b13",
   "metadata": {},
   "source": [
    "## 4. Dataloaders (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(root, img_size=IMG_SIZE, batch_size=BATCH_SIZE, seed=SEED):\n",
    "    train_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        root/\"train\", image_size=img_size, batch_size=batch_size, label_mode=\"categorical\", seed=seed, shuffle=True)\n",
    "    val_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        root/\"val\", image_size=img_size, batch_size=batch_size, label_mode=\"categorical\", seed=seed, shuffle=False)\n",
    "    test_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        root/\"test\", image_size=img_size, batch_size=batch_size, label_mode=\"categorical\", seed=seed, shuffle=False)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "# train_ds, val_ds, test_ds = make_datasets(DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12017a9",
   "metadata": {},
   "source": [
    "### 4.1 Augmentation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb620108",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065f386",
   "metadata": {},
   "source": [
    "## 5. Model: Transfer Learning Baseline (MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92318835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes=len(CLASSES), img_size=IMG_SIZE):\n",
    "    base = keras.applications.MobileNetV2(\n",
    "        input_shape=img_size + (3,), include_top=False, weights=\"imagenet\")\n",
    "    base.trainable = False  # fine-tune later\n",
    "\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "    x = data_augmentation(inputs)\n",
    "    x = keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "    x = base(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# model = build_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8780aea",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ecccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "# train_ds, val_ds, test_ds = make_datasets(DATASET_ROOT)\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=EPOCHS\n",
    "# )\n",
    "# pd.DataFrame(history.history).to_csv(\"training_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43196b1d",
   "metadata": {},
   "source": [
    "### 6.1 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history_dict):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    acc = history_dict.get(\"accuracy\", [])\n",
    "    val_acc = history_dict.get(\"val_accuracy\", [])\n",
    "    loss = history_dict.get(\"loss\", [])\n",
    "    val_loss = history_dict.get(\"val_loss\", [])\n",
    "    epochs_range = range(1, len(acc)+1)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.plot(epochs_range, acc, label=\"train_acc\")\n",
    "    if val_acc: plt.plot(epochs_range, val_acc, label=\"val_acc\")\n",
    "\n",
    "    # Loss on twin y-axis for clarity\n",
    "    ax2 = plt.gca().twinx()\n",
    "    ax2.plot(epochs_range, loss, linestyle=\"--\", label=\"train_loss\")\n",
    "    if val_loss: ax2.plot(epochs_range, val_loss, linestyle=\"--\", label=\"val_loss\")\n",
    "\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example after training:\n",
    "# plot_curves(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d6c51",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a526bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_ds, class_names=CLASSES):\n",
    "    y_true, y_prob = [], []\n",
    "    for x, y in test_ds:\n",
    "        p = model.predict(x, verbose=0)\n",
    "        y_prob.append(p)\n",
    "        y_true.append(y.numpy())\n",
    "    y_prob = np.concatenate(y_prob, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_labels, y_pred, labels=list(range(len(class_names))))\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(class_names)), class_names)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nClassification Report\\n\")\n",
    "    print(classification_report(y_true_labels, y_pred, target_names=class_names))\n",
    "\n",
    "    # One-vs-Rest ROC-AUC (if at least 2 classes)\n",
    "    if y_prob.shape[1] >= 2:\n",
    "        try:\n",
    "            auc_macro = roc_auc_score(y_true, y_prob, multi_class=\"ovr\")\n",
    "            print(f\"Macro ROC-AUC: {auc_macro:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(\"ROC-AUC could not be computed:\", e)\n",
    "\n",
    "# evaluate_model(model, test_ds, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ae08e",
   "metadata": {},
   "source": [
    "## 8. Explainability: Gradâ€‘CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array, training=False)\n",
    "        class_idx = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, class_idx]\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0) / (np.max(heatmap) + 1e-8)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Example usage after training:\n",
    "# target_layer = [l.name for l in model.layers if isinstance(l, layers.Conv2D)][-1]\n",
    "# img_path = next((DATASET_ROOT/'test'/'glioma').glob(\"*.jpg\"))\n",
    "# img = Image.open(img_path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "# x = np.expand_dims(np.array(img)/255.0, 0).astype(np.float32)\n",
    "# heatmap = make_gradcam_heatmap(x, model, last_conv_layer_name=target_layer)\n",
    "# plt.figure(figsize=(6,3))\n",
    "# plt.subplot(1,2,1); plt.imshow(img); plt.title(\"Image\"); plt.axis(\"off\")\n",
    "# plt.subplot(1,2,2); plt.imshow(heatmap); plt.title(\"Grad-CAM\"); plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3550f4",
   "metadata": {},
   "source": [
    "## 9. (Optional) Advanced Slot: DBN + BiLSTM\n",
    "This section is a placeholder where you can prototype a DBN feature extractor (via stacked RBMs) and feed the extracted features into a BiLSTM classifier over slice sequences. Implementations vary; consider using PyTorch or TensorFlow Probability for RBM-like layers, or substitute with an autoencoder as a practical surrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode / sketch (fill as needed):\n",
    "# 1) Build/Train stacked RBMs (or autoencoder) to extract features from each image/slice.\n",
    "# 2) Aggregate per-patient sequences: shape [time/slices, feature_dim].\n",
    "# 3) Feed sequences to a BiLSTM for classification.\n",
    "# 4) Train end-to-end (optionally fine-tune feature extractor).\n",
    "\n",
    "# from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Input, Masking\n",
    "# seq_inputs = Input(shape=(None, feature_dim))\n",
    "# x = Bidirectional(layers.LSTM(128, return_sequences=False))(seq_inputs)\n",
    "# outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "# seq_model = keras.Model(seq_inputs, outputs)\n",
    "# seq_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f729692",
   "metadata": {},
   "source": [
    "## 10. Export & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save class mapping & configuration\n",
    "config = {\n",
    "    \"classes\": CLASSES,\n",
    "    \"img_size\": IMG_SIZE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "with open(\"run_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"Saved run_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12998adc",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Ensure dataset paths are correct before running the dataloader cell.\n",
    "- Start with the baseline model; once stable, enable fineâ€‘tuning (`base.trainable=True`) and reâ€‘train with a lower LR.\n",
    "- Use the Gradâ€‘CAM cell to sanityâ€‘check that the network is focusing on plausible tumor regions.\n",
    "- Replace MobileNetV2 with any backbone (e.g., ResNet50, EfficientNet) if you prefer.\n",
    "- For publications, record seeds, exact package versions, and hardware.\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Generated by ChatGPT"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
